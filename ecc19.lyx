#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ieeeconf
\begin_preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

% Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
 Gaussian Process Based Model-free Control with Q-Learning*
}


\author{Jan Hauser$^{1}$, Daniel Pachner$^{2}$ and Vladimír Havlena$^{1}$% <-this % stops a space
\thanks{*This work has been supported by the project 18-26278S sponsored by Grant Agency of the Czech Republic.}% <-this % stops a space
\thanks{$^{1}$Jan Hauser and Vladimír Havlena are with Department of Control Engineering, 
Faculty of Electrical Engineering of Czech Technical University in Prague, Technicka 2, 166 27 Praha 6, Czech Republic.
Email: {\tt\small \{hauseja3,havlena\}@fel.cvut.cz}}%
\thanks{$^{2}$Daniel Pachner is with Honeywell HBT Architecture $\&$ Innovation Team, 
V Parku 2326/18, 148 00 Prague, Czech Republic 
Email: {\tt\small daniel.pachner@honeywell.com}}%
}
\end_preamble
\options conference
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_title "Gaussian Process Based Model-less Control with Q-Learning"
\pdf_author "Jan Hauser and Daniel Pachner and Vladimír Havlena"
\pdf_keywords "Gaussian process, Q-Learning, Model-less Controll, Machine Learning"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

{
\end_layout

\end_inset

empty
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
The aim of this paper is to demonstrate a new algorithm for Machine Learning
 (ML) based on Gaussian Process Regression (GPR) and how it can be used
 as a practical control design technique.
 An optimized control law for a nonlinear process is found directly by training
 the algorithm on noisy data collected from the process when controlled
 by a sub-optimal controller.
 A simplified nonlinear Fan Coil Unit (FCU) model is used as an example
 for which the fan speed control is designed using the off-policy Q-learning
 algorithm.
 Additionally, the algorithm properties are discussed, i.e.
 learning process robustness, GP kernel functions choice.
 The simulation results are compared to a simple PI, designed based on a
 linearized model.
\end_layout

\begin_layout Section
INTRODUCTION
\end_layout

\begin_layout Standard
Model-free control techniques assume that no mathematical model of the controlle
d process is available and the controller is designed from the measurement
 data.
 One such approach would collect the data in advance during some time window
 to use it offline for a controller design.
 A different approach would attempt to use the data in the real time to
 improve the control continuously.
 In this article the former offline approach is considered, i.e.
 the situation when some sub-optimal controller was already in use and the
 data were collected and can be used to optimize or improve that controller.
 Many existing control design techniques first create a model from data
 to use it for a control design method afterwards, which makes sense if
 some reliable modeling information, e.g.
 model structure, is available.
 A different approach, used in this paper, is the controller designed directly
 from the data, without any process model.
 This approach can have some advantages especially if little or nothing
 is known about the process or if the process is nonlinear and no analytical
 control design method is available.
\end_layout

\begin_layout Standard
The Q-learning is an off-policy machine learning (ML) iterative algorithm
 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Sutton_Reinforcement_Learning"
literal "true"

\end_inset

, which approximates certain function satisfying the Bellman equation.
 This Q function then defines a controller.
 Q-learning was developed for Markov Decision Process (MDP) with finite
 number of states and later generalized to continuous state spaces 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Hasselt_Reinforcement_learning_in_continuous_action_spaces,ecc19ref:Gaskett_Q_Learning_IC"
literal "false"

\end_inset

.
 If the analytical form of the Q function is unknown in continuous state
 spaces, it may be represented by an universal function approximating method
 such as Neural Network or Gaussian Process Regression (GPR).
\end_layout

\begin_layout Standard
GPR is a non-parametric regression technique 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Rasmussen_Gaussian_Processes"
literal "true"

\end_inset

 which is able to approximate any continuous target function uniformly.
 Unfortunately, the runtime computational requirement is 
\begin_inset Formula $\mathcal{O}(n^{3})$
\end_inset

 and the memory requirement is 
\begin_inset Formula $\mathcal{O}(n^{2})$
\end_inset

 for 
\begin_inset Formula $n$
\end_inset

 data points.
 Various sparse GPR techniques were developed to overcome this computational
 complexity 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Bijl_Online_Sparse_Gaussian,ecc19ref:Candela_A_unifying_view,ecc19ref:Huber_Recursive_GP"
literal "false"

\end_inset

.
 One of the conventional GPR sparse method is to use a set of size 
\begin_inset Formula $m$
\end_inset

 induction input points, which reduce the computational complexity to 
\begin_inset Formula $\mathcal{O}(nm^{2})$
\end_inset

 (runtime) and 
\begin_inset Formula $\mathcal{O}(nm)$
\end_inset

 (memory).
 These induction points reduce the whole dataset into a smaller number of
 representing data points, which are distributed across the data space to
 preserve as much information as possible.
 GP uses various covariance (kernel) functions 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Williams_Prediction_with_Gaussian_processes"
literal "false"

\end_inset

 to define the data covariance matrices.
 The choice of the kernel can have significant impact on the accuracy of
 the regression model.
\end_layout

\begin_layout Standard
The contribution of this paper is the practical and efficient combination
 of Q-learning and GPR resulting in unbiased estimate of the Q function.
 Often the training data are simulated by a model and the statistical properties
 of the algorithm are thus less important.
 Here a smaller dataset from a process affected by unmeasured disturbances
 is targeted (of a size 
\begin_inset Formula $\sim10^{3}$
\end_inset

).
 This requires the information in the data to be used efficiently.
 
\end_layout

\begin_layout Standard
A simple Fan Coil Unit (FCU) model approximation is used to demonstrate
 the approach.
 FCU is a nonlinear system widely used for both air heating and cooling
 in buildings.
 A linear control design cannot achieve optimal FCU control in terms of
 energy consumption and user comfort 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Arguello_Serrano_Nonlinear_HVAC"
literal "true"

\end_inset

.
 FCU model used here is highly simplified to make the result easier to interpret.
 It is supposed that ML will be able to optimize a real FCU control based
 on several days data.
\end_layout

\begin_layout Standard
For reader's understanding, some essential theory is briefly introduced.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:BACKGROUND"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the other papers from this stream are commented, then the GPR algorithm
 and the Q-learning mechanism are briefly described.
 Following Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Q-LEARNING-WITH-GPR"
plural "false"
caps "false"
noprefix "false"

\end_inset

 describes the main contribution and novelty of this paper, the unbiased
 Q-learning algorithm using GPR.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Fan-Coil-Unit"

\end_inset

, a reduced FCU model is explained.
 Next Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:RESULTS"

\end_inset

 presents the results of these techniques and Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:CONCLUSIONS"

\end_inset

 concludes and proposes a future work.
 
\end_layout

\begin_layout Section
BACKGROUND
\begin_inset CommandInset label
LatexCommand label
name "sec:BACKGROUND"

\end_inset


\end_layout

\begin_layout Standard
In order to understand presented algorithm properly, the GP and Q-learning
 principles are presented here as a background material.
 Also related work is commented here in this section.
\end_layout

\begin_layout Subsection
GAUSSIAN PROCESS REGRESSION
\begin_inset CommandInset label
LatexCommand label
name "sec:gp"

\end_inset


\end_layout

\begin_layout Standard
GPR is a supervised learning regression model, which can be also described
 as a distribution over functions 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Rasmussen_Gaussian_Processes"
literal "true"

\end_inset

.
 It is the function value estimator for an unknown function 
\begin_inset Formula $f(\mathbf{x})$
\end_inset

 considering any dataset 
\begin_inset Formula $(\mathbf{X},\mathbf{y})$
\end_inset

, which can be written as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f(\mathbf{x})\sim\mathcal{GP}\left(m(\mathbf{x}),\kappa(\mathbf{x},\mathbf{x}')\right),
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where mean 
\begin_inset Formula $m(\mathbf{x})$
\end_inset

 and covariance function 
\begin_inset Formula $\kappa(\mathbf{x},\mathbf{x}'$
\end_inset

) are given a priori up to some hyperparameters and are defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
m(\mathbf{x)} & = & \mathbb{E}\left[f(\mathbf{x})\right],\\
\kappa(\mathbf{x},\mathbf{x}') & = & \mathbb{E}\left[\left(f(\mathbf{x})-m(\mathbf{x})\right)\left(f(\mathbf{x}')-m(\mathbf{x}')\right)^{\top}\right],
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbb{E}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 is the expectation, 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}'$
\end_inset

 are a pair of vectors in the data space.
 A dataset is a number of such vectors 
\begin_inset Formula $\mathbf{X}=\left\{ \mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{n}\right\} $
\end_inset

.
 In this article 
\begin_inset Formula $f(\mathbf{x})\in\mathbb{R}.$
\end_inset


\end_layout

\begin_layout Standard
Assume the finite training set 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and the finite testing set 
\begin_inset Formula $\mathbf{X}_{p}$
\end_inset

, then GPR can predict 
\begin_inset Formula $z_{j}=f(\mathbf{x}_{p_{j}}^{\mathrm{}})$
\end_inset

, where 
\begin_inset Formula $\mathbf{x}_{p_{j}}^{\mathrm{}}\in\mathbf{X}_{p}$
\end_inset

, by using data 
\begin_inset Formula $\mathbf{x}_{i}\in\mathbf{X}$
\end_inset

 and their function values 
\begin_inset Formula $f(\mathbf{x}_{i})$
\end_inset

.
 The function values 
\begin_inset Formula $f(\mathbf{X})$
\end_inset

 themselves do not need to be accessible but rather their noisy measurements
 
\begin_inset Formula $y_{i}=f(\mathbf{x}_{i})+\varepsilon_{i}$
\end_inset

, where 
\begin_inset Formula $\varepsilon_{i}$
\end_inset

 is independent identically distributed (i.i.d) Gaussian noise with variance
 
\begin_inset Formula $\sigma_{n}^{2}$
\end_inset

.
 The prior covariance of the noisy values is defined as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathrm{cov}(\mathbf{y})=\kappa(\mathbf{X},\mathbf{X})+\sigma_{n}^{2}\mathbf{I}.\label{eq:covariance-noisy-measurements}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Then the prior joint probability distribution function (p.d.f) can be defined
 for training and testing sets values as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline*}
\left[\begin{array}{c}
\mathbf{y}\\
\mathbf{z}
\end{array}\right]\sim\left[\begin{array}{c}
f(\mathbf{X})+\boldsymbol{\varepsilon}\\
f(\mathbf{X}_{p})
\end{array}\right]\sim\\
\mathcal{N}\left(\left[\begin{array}{c}
m(\mathbf{X})\\
m(\mathbf{X}_{p})
\end{array}\right],\left[\begin{array}{cc}
\kappa(\mathbf{X},\mathbf{X})+\sigma_{n}^{2}\mathbf{I} & \kappa(\mathbf{X},\mathbf{X}_{p})\\
\kappa(\mathbf{X}_{p},\mathbf{X}) & \kappa(\mathbf{X}_{p},\mathbf{X}_{p})
\end{array}\right]\right)=\\
\mathcal{N}\left(\left[\begin{array}{c}
\mathbf{m}\\
\mathbf{m}_{z}
\end{array}\right],\left[\begin{array}{cc}
\mathbf{K}_{ff}+\sigma_{n}^{2}\mathbf{I} & \mathbf{K}_{fz}\\
\mathbf{K}_{zf} & \mathbf{K}_{zz}
\end{array}\right]\right),
\end{multline*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where 
\begin_inset Formula $\mathcal{N}$
\end_inset

 is normal distribution defined by mean and covariance, 
\begin_inset Formula $\mathbf{K}_{fz}$
\end_inset

 is a 
\begin_inset Formula $n\times n_{p}$
\end_inset

 matrix of the covariances of all pairs of training and testing datasets,
 and 
\begin_inset Formula $\mathbf{K}_{ff}$
\end_inset

, 
\begin_inset Formula $\mathbf{K}_{zz}$
\end_inset

, 
\begin_inset Formula $\mathbf{K}_{zf}$
\end_inset

 analogously.
 Let's also use a notation 
\begin_inset Formula $\mathbf{K}_{yy}=\mathbf{K}_{ff}+\sigma_{n}^{2}\mathbf{I}$
\end_inset

 for covariance of noisy measurements 
\begin_inset Formula $\mathbf{y}$
\end_inset

 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:covariance-noisy-measurements"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 There are many useful covariance functions 
\begin_inset Formula $\kappa(\mathbf{x},\mathbf{x}')$
\end_inset

 called kernels, e.g.
 squared exponential (SE)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\kappa(\mathbf{x},\mathbf{x}') & =\sigma_{f}^{2}\exp\left(-\frac{1}{2}\left(\mathbf{x}-\mathbf{x}'\right)\mathbf{\Lambda}^{-1}\left(\mathbf{x}-\mathbf{x}'\right)\right),\\
\mathbf{\Lambda} & =\mathrm{diag}(\lambda_{1}^{2},\ldots,\lambda_{n}^{2}),
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 or polynomial kernel of 
\begin_inset Formula $d$
\end_inset

-degree
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\kappa(\mathbf{x},\mathbf{x}')=\left(\mathbf{x}^{\top}\mathbf{x}'+c\right)^{d},
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where 
\begin_inset Formula $c\geq0$
\end_inset

.
 These kernel functions are also scalable by their hyperparameters, i.e.
 these are signal variance 
\begin_inset Formula $\sigma_{f}^{2}$
\end_inset

 and length-scale 
\begin_inset Formula $\mathbf{\Lambda}$
\end_inset

 for SE kernel or degree 
\begin_inset Formula $d$
\end_inset

 and soft-margin 
\begin_inset Formula $c$
\end_inset

 for polynomial kernel.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is known, then the posterior conditional normal distribution of 
\begin_inset Formula $\mathbf{z}$
\end_inset

 can be defined.
 The predictive GPR relationships are following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray}
p(\mathbf{z}|\mathbf{y}) & = & \mathcal{N}(\boldsymbol{\mu}_{z},\mathbf{\Sigma}_{z}),\nonumber \\
\boldsymbol{\mu}_{z} & = & \mathbb{E}\left[\mathbf{z}|\mathbf{y}\right]=\mathbf{m}_{z}+\mathbf{K}_{zf}\mathbf{K}_{yy}^{-1}(\mathbf{y}-\mathbf{m}),\label{eq:gp-posterior-mean}\\
\mathbf{\Sigma}_{z} & = & \mathbf{K}_{zz}-\mathbf{K}_{zf}\mathbf{K}_{yy}^{-1}\mathbf{K}_{fz},\label{eq:gp-posterior-cov}
\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where 
\begin_inset Formula $\boldsymbol{\mu}_{z}$
\end_inset

 is a mean vector and 
\begin_inset Formula $\mathbf{\Sigma}_{z}$
\end_inset

 is a covariance matrix.
\end_layout

\begin_layout Standard
There is an important relationship between the Kalman filter equations and
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gp-posterior-mean"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gp-posterior-cov"
plural "false"
caps "false"
noprefix "false"

\end_inset

) which will be used later.
 Specifically, we remind that the term 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{G}=\mathbf{K}_{zf}\mathbf{K}_{yy}^{-1}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 is the Kalman gain matrix.
 Assuming the unknown continuous function 
\begin_inset Formula $f$
\end_inset

 is a GP, then training points from dataset 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and the observed function values 
\begin_inset Formula $\mathbf{y}$
\end_inset

 define the posterior expectations (predictions) 
\begin_inset Formula $\mathbf{z}$
\end_inset

 for any test points over a dataset 
\begin_inset Formula $\mathbf{X}_{p}$
\end_inset

.
 Unfortunately, these simple calculations can get very expensive due to
 the inverse of matrix 
\series bold

\begin_inset Formula $\mathbf{K}_{yy}$
\end_inset


\series default
, which is of size 
\begin_inset Formula $n\times n$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the number of training data points.
 This is the operation which costs 
\begin_inset Formula $\mathcal{O}(n^{3})$
\end_inset

.
 This is the moment when sparse GPR is taken into account 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Bijl_Online_Sparse_Gaussian"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Q-LEARNING
\begin_inset CommandInset label
LatexCommand label
name "sec:Q-Learning"

\end_inset


\end_layout

\begin_layout Standard
This section describes the basic principles of Q-learning algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Sutton_Reinforcement_Learning"
literal "true"

\end_inset

, which is a model-free off-policy reinforcement learning approach.
 Then the generalized policy iteration algorithm based on Bellman equation
 is pointed out.
 
\end_layout

\begin_layout Standard
For purpose of this section, let's highlight the analogies and slight difference
s between two closely related fields: Optimal Control Theory (OCT), and
 MDP respectively.
 At a discrete time 
\begin_inset Formula $k$
\end_inset

, the vectors of states 
\begin_inset Formula $\mathbf{x}_{k}\in\mathcal{X}$
\end_inset

 and inputs 
\begin_inset Formula $\mathbf{u}_{k}\in\mathcal{U}$
\end_inset

 are usually considered in OCT for a process model, whereas MDP uses the
 Markov process states 
\begin_inset Formula $\mathbf{s}_{k}\in\mathcal{S}$
\end_inset

 and the agent's actions 
\begin_inset Formula $\mathbf{a}_{k}\in\mathcal{A}$
\end_inset

 analogously.
 Note that the sets 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathcal{X}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 and 
\begin_inset Formula $\mathcal{U}$
\end_inset

 are usually real vector spaces in control problems whereas 
\begin_inset Formula $\mathcal{S}$
\end_inset

 and 
\begin_inset Formula $\mathcal{A}$
\end_inset

 may often be finite sets in MDP.
 The process model itself is an analogy of probability transition matrix
 
\begin_inset Formula $p(\mathbf{s}_{k+1}|\mathbf{a}_{k},\mathbf{s}_{k})$
\end_inset

 of MDP.
 Control law, or a state feedback 
\begin_inset Formula $\mathbf{u}_{k}=C(\mathbf{x}_{k})$
\end_inset

 in OCT is an analogy of a deterministic policy 
\begin_inset Formula $\mathbf{a}_{k}=\pi(\mathbf{s}_{k})$
\end_inset

.
 A stochastic policy, usually not used in OCT, defines the joint p.d.f.
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\pi(\mathbf{a}_{k},\mathbf{s}_{k})$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 instead of an explicit function.
 An important difference exists between the reward 
\begin_inset Formula $r(\mathbf{a}_{k},\mathbf{s}_{k},\mathbf{s}_{k+1})$
\end_inset

 used in MDP (bounded, to be maximized) and loss function 
\begin_inset Formula $\ell(\mathbf{x}_{k},\mathbf{u}_{k})$
\end_inset

 used in OCT (not bounded in general, to be minimized, almost never depending
 on 
\begin_inset Formula $\mathbf{x}_{k+1}$
\end_inset

).
 The ML theory will be discussed below mostly with OCT notations and assumptions.
\end_layout

\begin_layout Subsubsection
Q Function
\end_layout

\begin_layout Standard
Generally, Q function is a scalar function of a state-input (state-action)
 pair, which maps to real values 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Q:\mathbf{\mathbf{u\times}x}\rightarrow\mathbb{R}.
\]

\end_inset


\end_layout

\begin_layout Standard
It is possible to talk either about Q function 
\begin_inset Formula $Q^{\pi}(\mathbf{u},\mathbf{x})$
\end_inset

 pertaining to a given policy 
\begin_inset Formula $\pi$
\end_inset

 or the function 
\begin_inset Formula $Q^{*}(\mathbf{x},\mathbf{u})$
\end_inset

, which pertains to the optimal policy 
\begin_inset Formula $\pi^{*}$
\end_inset

.
 
\begin_inset Formula $Q$
\end_inset

 (and 
\begin_inset Formula $Q^{*}$
\end_inset

) describes the expected total discounted loss received by the controller
 starting from 
\begin_inset Formula $\mathbf{x}$
\end_inset

 with a control action 
\begin_inset Formula $\mathbf{u}$
\end_inset

 and following with the policy 
\begin_inset Formula $\pi$
\end_inset

 (optimal 
\begin_inset Formula $\pi^{*}$
\end_inset

) thereafter.
 
\begin_inset Formula $Q^{*}$
\end_inset

, as function of 
\begin_inset Formula $\mathbf{u}$
\end_inset

, is thus a measure of quality of selecting the control action 
\begin_inset Formula $\mathbf{u}$
\end_inset

 in a given state 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 
\begin_inset Formula $Q^{*}$
\end_inset

 is minimized by the optimal control action(s) because it can only be made
 worse.
 There is also an important parallel between 
\begin_inset Formula $Q$
\end_inset

 function and the value (cost-to-go) function 
\begin_inset Formula $V$
\end_inset

 used in Dynamic Programming.
 It is also related to Lyapunov function and stability theory.
 
\begin_inset Formula $V$
\end_inset

 is not used for purpose of this paper.
 For a policy 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\pi$
\end_inset

, not necessarily optimal, and an instantaneous loss 
\begin_inset Formula $\ell(\mathbf{u}_{k},\mathbf{x}_{k})$
\end_inset

 at time 
\begin_inset Formula $k\in\mathbb{N}$
\end_inset

, the Q is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
Q^{\pi}(\mathbf{u}_{k},\mathbf{x}_{k})=\\
l(\mathbf{u}_{k},\mathbf{x}_{k})+\mathbb{E}\left[\left.\sum_{i=1}^{\infty}\gamma^{i}\ell(\mathbf{u}_{k+i}^{\pi},\mathbf{x}_{k+i})\right|\mathbf{u}_{k},\mathbf{x}_{k}\right],\label{eq:q-split-sum}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where 
\begin_inset Formula $\gamma\in\left(0,1\right]$
\end_inset

 is a discount factor.
 
\begin_inset Formula $Q^{*}$
\end_inset

 is defined as follows
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
Q^{*}(\mathbf{u}_{k},\mathbf{x}_{k})=\\
l(\mathbf{u}_{k},\mathbf{x}_{k})+\mathbb{E}\left[\left.\min_{\mathbf{u}_{k+i}}\sum_{i=1}^{\infty}\gamma^{i}\ell(\mathbf{u}_{k+i},\mathbf{x}_{k+i})\right|\mathbf{u}_{k},\mathbf{x}_{k}\right].\label{eq:q-function-star}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
The relationship between the function 
\begin_inset Formula $Q^{*}$
\end_inset

 and the optimal policy 
\begin_inset Formula $\pi^{*}$
\end_inset

 is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\pi^{*}(\mathbf{x})=\arg\min_{\mathbf{u}}Q^{*}(\mathbf{u},\mathbf{x}).\label{eq:q-function-policy}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The Bellman equation (more usually expressed in terms of 
\begin_inset Formula $V$
\end_inset

) provides the recursive approach for finding the functions 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $Q^{\pi}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 and 
\begin_inset Formula $Q^{*}$
\end_inset

.
 It follows directly from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:q-split-sum"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:q-function-star"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 For an optimal policy 
\begin_inset Formula $\pi^{*}$
\end_inset

, function 
\begin_inset Formula $Q^{*}(x_{k},u_{k})$
\end_inset

 must satisfy
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
Q^{*}(\mathbf{u}_{k},\mathbf{x}_{k})=\\
\ell(\mathbf{u}_{k},\mathbf{x}_{k})+\gamma\mathbb{E}\left[\left.\min_{\mathbf{u}_{k+1}}Q^{*}(\mathbf{u}_{k+1},\mathbf{x}_{k+1})\right|\mathbf{u}_{k},\mathbf{x}_{k}\right],\label{eq:q-value-iteration}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
and for general policy 
\begin_inset Formula $\pi$
\end_inset

 analogously by using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:q-split-sum"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Subsubsection
Generalized Policy Iteration
\begin_inset CommandInset label
LatexCommand label
name "subsec:Policy-Iteration"

\end_inset


\end_layout

\begin_layout Standard
Generalized Policy iteration (GPI) algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Sutton_Reinforcement_Learning"
literal "true"

\end_inset

 calculates Q function from Bellman equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:q-value-iteration"
plural "false"
caps "false"
noprefix "false"

\end_inset

) for current policy and then improves the current policy by seeking for
 minimum values of 
\begin_inset Formula $Q^{\pi}$
\end_inset

 with respect to 
\begin_inset Formula $\mathbf{u}$
\end_inset

 for each 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Such minimizing 
\begin_inset Formula $\mathbf{u}$
\end_inset

 defines the new policy.
 This process is repeated until convergence to 
\begin_inset Formula $Q^{*}$
\end_inset

.
 The starting policy is selected as stabilizing in order to ensure the initial
 
\begin_inset Formula $Q^{\pi}$
\end_inset

 is finite.
\end_layout

\begin_layout Subsection
RELATED WORK
\end_layout

\begin_layout Standard
There are many related papers presented in this article stream.
 Various of them uses the GP for on-policy learning, an example of such
 approach is the commonly used 
\emph on
state action reward state action
\emph default
 or SARSA 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Sutton_Reinforcement_Learning"
literal "false"

\end_inset

.
 For these algorithms the value function is learned for the same policy
 the samples are collected from.
 On the other hand, the off-policy learning of value function directly approxima
tes the optimal value function regardless the policy samples are collected
 from, still there could be f.e.
 a requirement for initial policy to be stable.
 An example of this behaviour is the already presented Q-learning algorithm
 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Chowdhary_Off_Policy"
literal "false"

\end_inset

.
 In this paper, Q-learning is selected in order to find optimal control
 strategy for a specific system.
 It is also used in combination with GP, which in our algorithm finds unbiased
 estimate of the Q function.
 The common usage of this combination results in biased estimate 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Engel_Bayes_Meets_Bellman,ecc19ref:Engel_Reinforcement_Learning_with_GP"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
There could be also found articles presenting model-based reinforcement
 learning algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Jung_GP_RMAXlike_Exploration,ecc19ref:Rasmussen_GP_in_RL"
literal "false"

\end_inset

 which depends on the model knowledge.
 In those papers, GP is commonly used not only for estimating the value
 function (Q function respectively), but also for estimating the model dynamics.
 It was found non-necessary for purpose of this paper, only input-output
 data are used in a batch form.
 Which leads us to the next common problem burdening this area.
 The difference between batch and online usage of the algorithms.
 For purpose of this paper, online approach is being investigated.
\end_layout

\begin_layout Standard
Last but not least, many of the papers are using some kind of sparsification
 methods to reduce the complexity of calculations.
 A commonly used sparsification approach is 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Csato_Sparse_Online_GP"
literal "false"

\end_inset

.
 This paper does not use sparsification, but considers it as possible way
 to lower the dimensionality.
 Presented algorithm enables to not to repeatedly calculate operations which
 are computationally heavy.
\end_layout

\begin_layout Section
Q-LEARNING WITH GPR
\begin_inset CommandInset label
LatexCommand label
name "sec:Q-LEARNING-WITH-GPR"

\end_inset


\end_layout

\begin_layout Standard
In this section, the unbiased Q-learning algorithm with GPR is introduced.
 For a finite (in number of states and actions) MDP, the Q-learning algorithms
 may approximate the Q function at every element of the state-action space
 using the observed samples 
\begin_inset Formula $\mathbf{x}_{k},\mathbf{u}_{k}$
\end_inset

, which were encountered during interaction with a system.
\end_layout

\begin_layout Standard
This paper considers GPR as a continuous 
\begin_inset Formula $Q^{\pi}$
\end_inset

 function approximation method and then optimizes the control action using
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Policy-Iteration"
plural "false"
caps "false"
noprefix "false"

\end_inset

) via numerical minimization.
 Let us define the set of training and prediction points for the GPR as
 concatenations of points in the state-action space
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{gather*}
\mathbf{X}_{p}^{\pi}=\left\{ \begin{array}{c}
\mathbf{u}_{2}^{\pi},\mathbf{x}_{2}\\
\vdots\\
\mathbf{u}_{k}^{\pi},\mathbf{x}_{k}^{\pi}
\end{array}\right\} ,\;\mathbf{X}=\left\{ \begin{array}{c}
\mathbf{u}_{1},\mathbf{x}_{1}\\
\vdots\\
\mathbf{u}_{k-1},\mathbf{x}_{k-1}
\end{array}\right\} .
\end{gather*}

\end_inset


\end_layout

\begin_layout Standard
Here 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a collection of state-action pairs visited by the process whereas 
\begin_inset Formula $\mathbf{X}_{p}^{\pi}$
\end_inset

 is a collection of states observed as results of the actions accompanied
 with the actions the evaluated strategy 
\begin_inset Formula $\pi$
\end_inset

 would presumably apply there.
 Note 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{X}_{p}^{\pi}$
\end_inset

 is known without actually applying the strategy 
\begin_inset Formula $\pi$
\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 That is why the approach may use a historical dataset to optimize 
\begin_inset Formula $\pi$
\end_inset

.
 Also, the concatenation of the losses will be used
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{\boldsymbol{\ell}}=\left[\begin{array}{c}
\ell(\mathbf{u}_{1},\mathbf{x}_{1})\\
\vdots\\
\ell(\mathbf{u}_{k-1},\mathbf{x}_{k-1})
\end{array}\right],
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 and let the Q function be a GP with known kernel.
 The notation 
\begin_inset Formula $\mathbf{f}$
\end_inset

 (shortened 
\begin_inset Formula $f(\mathbf{X})$
\end_inset

), 
\begin_inset Formula $\mathbf{z}$
\end_inset

 for the vectors of unknown function values and 
\begin_inset Formula $\mathbf{y}$
\end_inset

 for the noisy observed values used in GPR context will be preserved
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{z}^{\pi}=Q^{\pi}(\mathbf{X}_{p}^{\pi}),\quad\mathbf{f}^{\pi}=Q^{\pi}(\mathbf{X}).
\]

\end_inset


\end_layout

\begin_layout Standard
The noisy realization of 
\begin_inset Formula $\mathbf{f}^{\pi}=\mathbb{\boldsymbol{\ell}+\gamma E}[Q^{\pi}(\mathbf{X}_{p}^{\pi})]$
\end_inset

 is 
\begin_inset Formula $\mathbf{y}^{\pi}=\mathbf{\boldsymbol{\ell}}+\gamma\mathbf{z}^{\pi}$
\end_inset

.
 Then based on (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gp-posterior-mean"
plural "false"
caps "false"
noprefix "false"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:q-value-iteration"
plural "false"
caps "false"
noprefix "false"

\end_inset

), the conditional means are
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
\mathbb{E\left[\mathit{\left.\begin{array}{c}
\mathbf{f}^{\pi}\\
\mathbf{z}^{\pi}
\end{array}\right|\mathbf{\mathit{\mathbf{y}}}^{\pi}}\right]}=\\
\left[\begin{array}{c}
\mathbf{m}\\
\mathbf{m}_{z}
\end{array}\right]+\left[\begin{array}{c}
\mathbf{K}_{ff}\\
\mathbf{K}_{zf}^{\pi}
\end{array}\right]\mathbf{K}_{yy}^{-1}\left(\mathbf{\boldsymbol{\ell}}+\gamma\mathbf{z}^{\pi}-\mathbf{m}\right).\label{eq:Q-data-update}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
The expression (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-data-update"
plural "false"
caps "false"
noprefix "false"

\end_inset

) may seem useless because the Q estimates depend on 
\begin_inset Formula $\mathbf{z}^{\pi}$
\end_inset

 value, which we do not know.
 Recall that Q function is neither measured nor observed.
 However, consider the left hand side equals to the true values 
\series bold

\begin_inset Formula $\mathbf{f}^{\pi}$
\end_inset


\series default
 and 
\begin_inset Formula $\mathbf{z}^{\pi}$
\end_inset

 for 
\begin_inset Formula $k\rightarrow\infty$
\end_inset

, sufficient excitation in the state-action space, and when 
\begin_inset Formula $\mathbf{m}=\mathbf{f}^{\pi}$
\end_inset

, 
\begin_inset Formula $\mathbf{m}_{z}=\mathbf{z}^{\pi}$
\end_inset

 respectively.
 The 
\series bold

\begin_inset Formula $\mathbf{f}^{\pi}$
\end_inset


\series default
 and 
\begin_inset Formula $\mathbf{z}^{\pi}$
\end_inset

 thus represent a fixed point of the following iterations (index 
\begin_inset Formula $\pi$
\end_inset

 dropped)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{multline}
\left[\begin{array}{c}
\mathbf{f}^{(i+1)}\\
\mathbf{z}^{(i+1)}
\end{array}\right]=\\
\left[\begin{array}{c}
\mathbf{f}^{(i)}\\
\mathbf{z}^{(i)}
\end{array}\right]+\left[\begin{array}{c}
\mathbf{K}_{ff}\\
\mathbf{K}_{zf}
\end{array}\right]\mathbf{K}_{yy}^{-1}\left(\boldsymbol{\ell}+\gamma\mathbf{z}^{(i)}-\mathbf{f}^{(i)}\right).\label{eq:GPR-iterations}
\end{multline}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 An estimate may be calculated starting the iterations from 
\begin_inset Formula $\mathbf{m},\mathbf{m}_{z}$
\end_inset

.
 Instead of actually iterating (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:GPR-iterations"
plural "false"
caps "false"
noprefix "false"

\end_inset

), a system of linear equations, which satisfy the fixed point values, can
 be solved.
 However, this system of linear equations will typically be ill-conditioned.
 In general, the update (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-data-update"
plural "false"
caps "false"
noprefix "false"

\end_inset

) does not uniformly reduce the uncertainty and the respective mapping is
 not a contraction.
 One may now use the Kalman filter and GPR analogy to understand that the
 latter happens because some linear combinations of the Q function values
 are not observable 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Kwakernaak_linear_optimal_control_systems"
literal "false"

\end_inset

 and some information from the starting values 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{m},\mathbf{m}_{z}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 does not vanish.
 We propose to regularize this situation by shifting the unobservable poles
 from 1 to some stable real pole 
\begin_inset Formula $1-\xi$
\end_inset

, i.e.
 inside the unit circle by 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\xi\geq0$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
.
 Practically, this means that the unobservable 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
Q function values will be estimated as zeros (
\begin_inset Formula $\mathbf{m}$
\end_inset

 could also be used).
 This regularization adds a fictitious time update step to the Kalman filter,
 shrinking the right hand side of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:GPR-iterations"
plural "false"
caps "false"
noprefix "false"

\end_inset

) by the factor of 
\begin_inset Formula $1-\xi$
\end_inset

.
 It results in the following estimates
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\left[\begin{array}{c}
\hat{\mathbf{f}}^{\pi}\\
\mathbf{\hat{z}}^{\pi}
\end{array}\right]=\left(\left(1-\xi\right)\left[\begin{array}{cc}
\mathbf{G}^{\pi}, & -\gamma\mathbf{G}^{\pi}\end{array}\right]-\xi\mathbf{I}\right)^{-1}\mathbf{G}^{\pi}\boldsymbol{\ell},\label{eq:Q-unbiased-estimate}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 with the Kalman gain matrix 
\begin_inset Formula $\mathbf{G^{\pi}}$
\end_inset

 defined as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{G}^{\pi}=\left[\begin{array}{c}
\mathbf{K}_{ff}\\
\mathbf{K}_{zf}^{\pi}
\end{array}\right]\mathbf{K}_{yy}^{-1}.
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

 Now imagine that the Q function value at a general query point 
\begin_inset Formula $\mathbf{\mathit{f}}_{q}^{\pi}=Q^{\pi}(\mathbf{u}_{k}^{q},\mathbf{x}_{k})$
\end_inset

 is also updated in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-data-update"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Such value may be queried by a numerical method trying to improve the current
 policy.
 This estimate may now be obtained reusing the above estimates 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\hat{\mathbf{f}}^{\pi},\mathbf{\hat{z}^{\pi}}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 as well as
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $\mathbf{K}_{yy}^{-1}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 and recalculating only a row kernel matrix 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{K}_{qf}^{\pi}$
\end_inset

 which is the only datum changed by the query point.
 The result is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{\hat{\mathit{f}}}_{q}^{\pi}=\mathbf{K}_{qf}^{\pi}\mathbf{K}_{yy}^{-1}\left(\frac{1-\xi}{\xi}\left(\hat{\mathbf{f}}^{\pi}-\gamma\mathbf{\hat{z}}^{\pi}\right)-\frac{1}{\xi}\boldsymbol{\ell}\right).\label{eq:Q-fast-update}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset

Without proofs, we state several statistical properties of the estimates
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-unbiased-estimate"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 They are unbiased (under GPR assumptions) except of the bias towards zero
 caused by 
\begin_inset Formula $\xi>0$
\end_inset

, see Fig.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Biased-vs-Unbiased"
plural "false"
caps "false"
noprefix "false"

\end_inset

) TBD.
 However, they are not optimal because 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{y}-\mathbf{f}$
\end_inset

 are in general not normal, independent and homeoskedastic.
 Also, it should be noted that the uncertainty of this estimate cannot be
 calculated by (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gp-posterior-cov"
plural "false"
caps "false"
noprefix "false"

\end_inset

), but must be estimated in a different way.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Biased-vs-Unbiased"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The flow of calculations is in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Q-learning-with-GP"

\end_inset

.
 In the first step, it calculates the kernel matrix 
\begin_inset Formula $\mathbf{K}_{ff}$
\end_inset

 and matrix inversion 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{K}_{yy}^{-1}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 for later usage as it depends only on the data, not the optimized policy.
 Then data matrix 
\begin_inset Formula $\mathbf{X}_{p}^{\pi^{(i)}}$
\end_inset

 and kernel matrix 
\begin_inset Formula $\mathbf{K}_{zf}^{\pi^{(i)}}$
\end_inset

 have to be updated according to actual policy 
\begin_inset Formula $\pi^{(i)}$
\end_inset

, starting with some known stabilizing policy 
\begin_inset Formula $\pi^{(1)}$
\end_inset

.
 
\begin_inset Formula $Q^{\pi^{(i)}}$
\end_inset

 estimate is also calculated for each policy 
\begin_inset Formula $\pi^{(i)}$
\end_inset

 by solving a linear system of equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-unbiased-estimate"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 All available data can be used in this step.
 This unbiased estimate of 
\begin_inset Formula $Q^{\pi^{(i)}}$
\end_inset

 is then minimized at each state 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{x}_{j}$
\end_inset

 using actions 
\begin_inset Formula $\mathbf{u}_{j}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 at step (
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:update-pi"
plural "false"
caps "false"
noprefix "false"

\end_inset

) in order to define a new improved policy 
\begin_inset Formula $\pi^{(i+1)}$
\end_inset

, where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{u}_{j},\mathbf{x}_{j}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 are predefined according to process/system.
 The minimization is iterative, evaluating the Q function using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-fast-update"
plural "false"
caps "false"
noprefix "false"

\end_inset

), which is an inner product.
 The stop condition is based on either a number of iterations limit or vanishing
 difference between the Q function values evaluated at last two policies
 
\begin_inset Formula $\pi^{(i)}$
\end_inset

 and 
\begin_inset Formula $\pi^{(i-1)}$
\end_inset

.
 Finally, the GP defines the optimal control action at any process state
 implicitly by (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:q-function-policy"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Box Boxed
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 1
use_makebox 0
width "3in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout

\series bold
Require: 
\begin_inset Formula $\mathbf{X},\mathbf{X}_{p}^{\pi^{(1)}},\boldsymbol{\ell},\epsilon,\gamma,\xi,\pi^{(1)},\mathbf{x}_{j},\mathbf{u}_{j}$
\end_inset


\end_layout

\begin_layout Enumerate
calculate kernel 
\begin_inset Formula $\mathbf{K}_{ff}$
\end_inset

 and kernel inversion 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{K}_{yy}^{-1}$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "enu:calculate-kernel-inv"

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $i=1$
\end_inset

,
\series bold
 repeat
\series default

\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Enumerate
update 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathbf{X}_{p}^{\pi^{(i)}}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 and kernel 
\begin_inset Formula $\mathbf{K}_{zf}^{\pi^{(i)}}$
\end_inset

 according to new 
\begin_inset Formula $\pi^{(i)}$
\end_inset


\begin_inset CommandInset label
LatexCommand label
name "enu:update-kalman-gain"

\end_inset


\end_layout

\begin_layout Enumerate
calculate 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\hat{\mathbf{f}}^{(i)},\mathbf{\hat{z}}^{(i)}$
\end_inset

 using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-unbiased-estimate"
plural "false"
caps "false"
noprefix "false"

\end_inset

)
\begin_inset CommandInset label
LatexCommand label
name "enu:calculate-Q-estimate"

\end_inset


\end_layout

\begin_layout Enumerate
improve policy
\begin_inset Newline newline
\end_inset


\begin_inset Formula $\pi^{(i+1)}(\mathbf{x}_{j})\leftarrow\arg\min_{\mathbf{u}_{j}}Q^{\pi^{(i)}}(\mathbf{u}_{j},\mathbf{x}_{j})$
\end_inset


\begin_inset Newline newline
\end_inset

using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Q-fast-update"
plural "false"
caps "false"
noprefix "false"

\end_inset

) 
\begin_inset CommandInset label
LatexCommand label
name "enu:update-pi"

\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $i=i+1$
\end_inset


\end_layout

\end_deeper
\begin_layout Enumerate

\series bold
until
\series default
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\max\left|\hat{\mathbf{f}}^{(i-1)}-\hat{\mathbf{f}}^{(i)}\right|<\epsilon$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Q-learning with GPR 
\begin_inset CommandInset label
LatexCommand label
name "alg:Q-learning-with-GP"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
FAN COIL UNIT 
\begin_inset CommandInset label
LatexCommand label
name "sec:Fan-Coil-Unit"

\end_inset


\end_layout

\begin_layout Standard
This section introduces the simplified FCU model used for testing the algorithm
 from previous section.
 FCU is a common air conditioning system, which is inherently non-linear
 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Arguello_Serrano_Nonlinear_HVAC"
literal "true"

\end_inset

.
 Usually installed in building interiors, it consists of a speed controllable
 electrical fan, a copper coil flown with heating and/or cooling liquid
 (a heat exchanger), and an air supply.
 It mixes the recirculated interior air with primary (outdoor) air.
 This air mixture is then heated/cooled according to the air temperature
 setpoint error by flowing through the coil.
 Then such air is supplied into the interior and mixed.
 The goal is to achieve the temperature set-point maintaining the interior
 
\begin_inset Formula $\mathrm{CO_{2}}$
\end_inset

 fraction and relative humidity at acceptable limits.
 Except of the obvious air heating and cooling effect, the heat supplied
 to or removed from the air can also be related to water evaporation or
 condensation in the unit.
 It thus makes a difference whether a FCU changes temperature of more air
 by less or vice versa.
 A model based optimal controller cannot be supplied by the unit manufacturer
 because the process model involves model of the interior, including its
 volume, thermal capacities, thermal insulation, solar and thermal load
 predictions, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
typical 
\begin_inset Formula $\mathrm{CO_{2}}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 and humidity loads.
 That is why model-free control or ML techniques may come into account.
 If such controllers could periodically re-optimize their behavior using
 ML techniques, significant amounts of energy could be saved world wide.
\end_layout

\begin_layout Subsection
Model 
\end_layout

\begin_layout Standard
Only the room air temperature 
\begin_inset Formula $T_{z}$
\end_inset

 
\begin_inset Formula $\left[\unit{\textdegree C}\right]$
\end_inset

 state is taken into consideration for purpose of this paper.
 Considering the perfect air mixing in the interior, it is described by
 the differential equation 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\dot{T}_{z}(t)=\frac{f(t)}{V}\left(T_{s}(t)-T_{z}(t)\right)+\frac{q_{L}(t)}{c_{p}V\rho},
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where 
\begin_inset Formula $T_{s}(t)$
\end_inset

 
\begin_inset Formula $\left[\unit{\textdegree C}\right]$
\end_inset

 is supply air temperature, 
\begin_inset Formula $q_{L}(t)$
\end_inset

 
\begin_inset Formula $\left[W\right]$
\end_inset

 is net heat load/loss, 
\begin_inset Formula $f(t)$
\end_inset

 
\begin_inset Formula $\left[\unit{m^{3}/\unit{s}}\right]$
\end_inset

 is air flow, 
\begin_inset Formula $V$
\end_inset

 
\begin_inset Formula $\left[\unit{m^{3}}\right]$
\end_inset

 is volume of the interior, 
\begin_inset Formula $\rho$
\end_inset

 
\begin_inset Formula $[\unit{kg/\unit{m^{3}}}]$
\end_inset

 is air density and 
\begin_inset Formula $c_{p}$
\end_inset


\begin_inset Formula $\left[\unit{J/\unit[kg]{K}}\right]$
\end_inset

 is air spec.
 thermal capacity.
 Let us define the volume independent control action as 
\begin_inset Formula $u(t)=\tau f(t)/V$
\end_inset

, i.e.
 the relative fraction of the air replaced per time unit 
\begin_inset Formula $\tau$
\end_inset

 (e.g.
 one hour).
 The supply air temperature 
\begin_inset Formula $T_{s}(t)$
\end_inset

 is a nonlinear function of air flow rate 
\begin_inset Formula $u(t)$
\end_inset

.
 The nonlinearity of 
\begin_inset Formula $T_{s}(t)$
\end_inset

 for purpose of this paper was approximated by the rational function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
T_{s}(t)=\frac{u(t)T_{z}(t)+eT_{0}}{u(t)+e},\label{eq:supply-air-temperature}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where 
\begin_inset Formula $e$
\end_inset

 is a heat exchanger size factor and 
\begin_inset Formula $T_{0}\left[\mathrm{\textdegree C}\right]$
\end_inset

 is the maximum supply air temperature.
 The maximum supply air temperature decreases from
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $T_{0}$
\end_inset

 (considered 
\begin_inset Formula $\mathrm{40^{o}C}$
\end_inset

)
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 to 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $T_{z}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 asymptotically when the air flow increases.
 For simplicity, we neglected the primary air.
 The heat losses were considered as 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\tau q_{L_{0}}/c_{p}V\rho=-7\mathrm{\textdegree C}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
, i.e.
 the room temperature would drop by this amount per unit time if the air-conditi
oning would be stopped.
\end_layout

\begin_layout Section
RESULTS
\begin_inset CommandInset label
LatexCommand label
name "sec:RESULTS"

\end_inset


\end_layout

\begin_layout Standard
This section presents the results.
 Model from Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Fan-Coil-Unit"

\end_inset

 was considered in discrete time form obtained by the Euler method with
 the sampling rate 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\tau/200$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
.
 The GPI algorithm was applied in order to find the optimal control policy.
 Loss function 
\begin_inset Formula $\ell$
\end_inset

 was defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\ell(u_{k},x_{k})=(T_{k}-T_{sp})^{2}+u_{k}^{4},\label{eq:loss-calculation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 where setpoint temperature was 
\begin_inset Formula $T_{sp}=22\unit{\textdegree C}$
\end_inset

 .
 GPR used the product of a polynomial kernel (degree two) and SE kernel
 as the kernel function.
 Recall that product of kernels is again a kernel.
 This choice is based on the fact known from linear control theory 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Kwakernaak_linear_optimal_control_systems"
literal "false"

\end_inset

: a linear model with quadratic loss has quadratic Q.
 Hence, our choice defines a locally linear control law.
\end_layout

\begin_layout Standard
Training dataset consists of 
\begin_inset Formula $2,000$
\end_inset

 points 
\begin_inset Formula $\sim10$
\end_inset

 hours.
 
\begin_inset Formula $T_{z_{k}}$
\end_inset

 is the only state 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $x_{k}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 of the process.
 See Fig.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Q-Learning-training-data"

\end_inset

).
 The data used for learning were generated by simulation.
 The control 
\begin_inset Formula $u_{k}$
\end_inset

 was selected as random bounded input.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/training_data.eps
	width 95col%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Q-Learning-training-data"

\end_inset

Q-learning training data consists of 
\begin_inset Formula $2,000$
\end_inset

 data points (
\begin_inset Formula $\sim10$
\end_inset

 hours) divided into 
\begin_inset Formula $15$
\end_inset

 continuous time intervals during which the room was heated from a cold
 condition.
 The room temperature 
\begin_inset Formula $T_{z_{k}}\sim x_{k}$
\end_inset

, supply air flow rate 
\begin_inset Formula $u_{k}$
\end_inset

, and also supply air temperature 
\begin_inset Formula $T_{s}$
\end_inset

 calculated from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:supply-air-temperature"

\end_inset

) are shown.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Q function was calculated by the proposed method and optimal control policy
 
\begin_inset Formula $\pi^{*}$
\end_inset

 was found after several policy iterations (around five suffice).
 Values 
\begin_inset Formula $\gamma=0.999$
\end_inset

 and 
\begin_inset Formula $\xi=10^{-5}$
\end_inset

 were used in the algorithm.
 It is important that 
\begin_inset Formula $u_{k}^{\pi}$
\end_inset

 must excite the process to cover the state-action space.
 The control actions must be partly randomized to get a valid training dataset.
 This is related to the well known problem of exploration-exploitation trade-off.
 Fig.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Q-Learning-with-Gaussian"

\end_inset

) shows the trajectory of optimal policy as a curve connecting the minima
 of Q function with respect to 
\begin_inset Formula $u$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/q_learning_improved.eps
	width 95col%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Q-Learning-with-Gaussian"

\end_inset

Q-learning with GPR and GPI.
 
\begin_inset Formula $Q^{*}$
\end_inset

 contours and policies 
\begin_inset Formula $\pi^{(1)},\pi^{(2)}$
\end_inset

 and 
\begin_inset Formula $\pi^{*}$
\end_inset

 (highlighted) calculated from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:q-function-policy"

\end_inset

).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This Q learning result makes good sense intuitively.
 The air exchange rate 
\begin_inset Formula $u_{k}$
\end_inset

 equals the heat losses when the room temperature is at the set point.
 Then it increases if the temperature is lower in order to heat the interior.
 Therefore, there is a negative feedback as expected.
 However, this feedback gain becomes smaller when the control error is greater
 because the large air flows are less effective for heating due to limited
 heat exchanger effectiveness and the decreasing supply air temperature.
 Instead, the electrical fan noise and the air flow would just annoy the
 occupants.
 Also, the fan would use more electricity.
 All this is modeled by the second term in 
\begin_inset Formula $\ell$
\end_inset

.
 As such, the policy resembles a proportional feedback controller with variable
 gain.
 It does not have any integral action whereas a proportional integral derivative
 (PID) controller would be normally used for similar purpose.
 However, it can be shown that the integral action can be added by augmenting
 the state space with temperature time difference and considering the time
 difference 
\begin_inset Formula $u_{k}-u_{k-1}$
\end_inset

 as the control action.
 Recall that the integral action is important in order to reject unmeasured
 slow disturbances.
\end_layout

\begin_layout Subsection
Q-learning Evaluation
\end_layout

\begin_layout Standard
The results from Q-learning were compared to multiple PI controllers in
 terms of the cumulative loss function 
\begin_inset Formula $L$
\end_inset

 values.
 This function represents the sum of all losses during an episode, i.e.
 
\begin_inset Formula $L=\sum_{k}\ell_{k}$
\end_inset

.
 An assuredly optimal value of cumulative loss 
\begin_inset Formula $L^{*}$
\end_inset

 is given by the optimal policy 
\begin_inset Formula $\pi^{*}$
\end_inset

 from Q learning.
 A grid of proportional and integral PI constants was considered so that
 it obviously contained the optimal PI values (local minimum).
 All 
\begin_inset Formula $L$
\end_inset

 values were calculated using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss-calculation"
plural "false"
caps "false"
noprefix "false"

\end_inset

), starting at the same initial condition 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathrm{10^{o}C}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 over next 1,000 sampling periods considering 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $T_{sp}=22\unit{\textdegree C}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
, so the results are comparable.
 These values were compared in Fig.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Comparison-of-PI-Q-Learning"

\end_inset

).
 The best PI controller parameters from the grid were selected for Fig.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Q-Learning-PI-PI"

\end_inset

).
 A PI controller designed for a linearized model of FCU at 
\begin_inset Formula $u_{0}=1\left[\unitfrac{1}{h}\right],x_{0}=22\left[\unit{\textdegree C}\right]$
\end_inset

 is also visualized.
 It can be observed that the best PI almost matches the result of the Q
 learning whereas the model linearization based result is different.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/q_learning_vs_pi_contr.eps
	width 95col%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Comparison-of-PI-Q-Learning"

\end_inset

Comparison of PI controllers cumulative losses 
\begin_inset Formula $L$
\end_inset

 with optimal policy cumulative loss 
\begin_inset Formula $L*$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/optimal_policy.eps
	width 95col%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Q-Learning-PI-PI"

\end_inset

Comparison of Q-learning optimal control policy, PI controller designed
 by cumulative loss comparison and PI controller designed from linearized
 model of FCU.
 The results are presented on nonlinear FCU model.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Next, the controllers and the Q learning were tested considering the net
 heat load/heat loss 
\begin_inset Formula $q_{L}$
\end_inset

 not constant but uniformly distributed over 
\begin_inset Formula $\left[-9,-5\right]$
\end_inset

.
 The result is shown in Fig.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Q-Learning-PI-PI-noisy"

\end_inset

).
 Note that Q-learning designed controller is robust towards such a noise.
 It should be noted that the same noise was used to generate the learning
 data for this test, not only when simulating the controller.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figures/optimal_policy_noisy.eps
	width 95col%

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Q-Learning-PI-PI-noisy"

\end_inset

Comparison of Q-learning optimal control policy, PI controller designed
 by cumulative loss comparison and PI controller designed from linearized
 model of FCU.
 The results are presented on nonlinear FCU model with noisy net heat load/heat
 loss 
\begin_inset Formula $q_{L}$
\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
CONCLUSIONS
\begin_inset CommandInset label
LatexCommand label
name "sec:CONCLUSIONS"

\end_inset


\end_layout

\begin_layout Standard
This paper described a practical approach of using unbiased GPR based Q-learning
 algorithm to find a control law for a completely unknown nonlinear process
 based on a historical dataset of a medium size 
\begin_inset Formula $\sim10^{3}$
\end_inset

.
 Engineers face such problem often and a solution is of practical interest.
 An efficient GPR approach was used to calculate an unbiased Q function
 value estimate in any point in the state-action space.
 The policy iterations were used to optimize the controller.
 This method typically converges rapidly.
 The optimal control law was then fully defined by the minima of a GP.
 This represents a numerical optimization in a low dimensional space of
 controller outputs and is thus numerically trackable.
 Although not explained in this paper, GP can be globally minimized even
 if it is not convex, see 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Franey_Branch_and_Bound_Algo"
literal "false"

\end_inset

.
 An unbiased Q estimate makes the method insensitive to noise affecting
 the process.
 Our approach is consistent with the Least-Squares Temporal Difference Learning
 (LSTD) 
\begin_inset CommandInset citation
LatexCommand cite
key "ecc19ref:Bratke_Linear_Least_Squares_Algo"
literal "false"

\end_inset

 using GPR.
 The approach can be integrated with the GPR sparse form in order to lower
 the dimensionality.
 However, details of this reduction are currently a subject of research.
 The approach of unbiased estimation was verified on a simple linear model
 against Q function calculated as a solution of Riccati equation, this verificat
ion is not part of the paper in detail.
 It was then tested on a simplified nonlinear one-input one-state FCU simulation
 model and an optimal control policy 
\begin_inset Formula $\pi^{*}$
\end_inset

 was calculated.
 The result makes sense intuitively, the feedback gain is gradually decreasing
 with the control error.
 A grid of PI controllers were compared in terms of cumulative loss 
\begin_inset Formula $L$
\end_inset

 towards 
\begin_inset Formula $L^{*}$
\end_inset

,  found by the Q-learning.
 The best PI controller from the grid was slightly worse than 
\begin_inset Formula $L^{*}$
\end_inset

.
 However, such direct controller search is impractical without a simulation
 model because it requires evaluating many controllers from defined initial
 conditions and affected by defined disturbances.
 Also, a PI controller was designed based on a linearized FCU model and
 PI tuning rules.
 This traditional approach could actually be used in practice together with,
 for example, Ziegler Nichols PID calibration method.
 The controllers were compared in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:RESULTS"

\end_inset

 and the performance of PI designed using linearized FCU was shown to be
 worse than both Q-learning and the PI found by direct search.
 Here the problem may be both linearization point and PID tuning rule which
 does not minimize the loss (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss-calculation"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Although the example was a single input single output control problem,
 the method applies to multidimensional problems without modifications.
 The whole process was described for reader's understanding on the high
 level.
 Many technical details were mentioned just briefly.
 However, the method is simple and straightforward.
\end_layout

\begin_layout Standard
The main pitfalls of the process may be also pointed out.
 It is necessary to choose a kernel function and several hyperparameters.
 However these choices affect the accuracy, the method should still converges
 to the same Q function asymptotically with growing dataset.
 The optimization of hyperparameters in connection with the proposed approach
 is our current research topic.
 Although result with only one kernel (SE times quadratic) was presented,
 different kernels were also tried with similar results, the hyperparameters
 were chosen reasonably and the kernels were smooth.
 The method assumes the process state is measured without error.
 This is not realistic in most control applications.
 The state may be often approximated by measurements and lagged measurements
 and control actions.
 Although the Q-learning seems to work well if the approximation is reasonable,
 an optimal state approximation is currently investigated in terms of dimension
 versus accuracy trade-off.
 The training dataset must provide exploration and cannot be obtained by
 running a fixed controller.
 A control action randomization is necessary.
 A stabilizing initial controller is required.
 This does not seem to be a serious constraint in many practical applications
 such as building control.
 
\end_layout

\begin_layout Standard
Overall, the method gives reasonably consistent results.
 Also note that theoretical proofs for declared statements regarding the
 used method are work in progress.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ecc19ref"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
